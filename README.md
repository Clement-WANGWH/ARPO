<div align="center">
  <img src="https://github.com/dongguanting/ARPO/blob/main/logo1.png" width="150px">
</div>

<h1 align="center" style="margin-top: -50px;">‚ú® Agentic Reinforced Policy Optimization</h1>


<div align="center"> 

[![Paper](https://img.shields.io/badge/Paper-arXiv-b5212f.svg?logo=arxiv)]()
[![Paper](https://img.shields.io/badge/Paper-Hugging%20Face-yellow?logo=huggingface)]()
[![Model](https://img.shields.io/badge/Model-Hugging%20Face-blue?logo=huggingface)](https://huggingface.co/collections/dongguanting/arpo-688229ff8a6143fe5b4ad8ae)
[![Dataset](https://img.shields.io/badge/Dataset-Hugging%20Face-blue?logo=huggingface)](https://huggingface.co/collections/dongguanting/arpo-688229ff8a6143fe5b4ad8ae)
[![License](https://img.shields.io/badge/LICENSE-MIT-green.svg)](https://opensource.org/licenses/MIT) 
[![Python 3.10+](https://img.shields.io/badge/Python-3.10+-blue.svg)](https://www.python.org/downloads/release/python-390/) 
[![X (formerly Twitter) URL](https://img.shields.io/twitter/url?url=https%3A%2F%2Fx.com%2FKevin_GuoweiXu%2Fstatus%2F1858338565463421244)]()
</div>

<!--
<p align="center">
ü§ó <a href="https://huggingface.co/dongguanting/Qwen2.5-3B-ARPO" target="_blank">Qwen2.5-3B-ARPO</a> ÔΩú
ü§ó <a href="https://huggingface.co/dongguanting/Qwen2.5-7B-ARPO" target="_blank">Qwen2.5-7B-ARPO</a> ÔΩú
ü§ó <a href="https://huggingface.co/dongguanting/Llama3.1-8B-ARPO" target="_blank">Llama3.1-8B-ARPO</a> ÔΩú
ü§ó <a href="https://huggingface.co/dongguanting/Qwen3-8B-ARPO-DeepSearch" target="_blank">Qwen3-8B-ARPO-DeepSearch</a> ÔΩú 
ü§ó <a href="https://huggingface.co/dongguanting/Qwen3-14B-ARPO-DeepSearch" target="_blank">Qwen3-14B-ARPO-DeepSearch</a> ÔΩú
</p>
<p align="center">
ü§ó <a href="https://huggingface.co/datasets/dongguanting/ARPO-SFT-54K" target="_blank">ARPO-SFT-54K</a> ÔΩú
ü§ó <a href="https://huggingface.co/datasets/dongguanting/ARPO-RL-Reasoning-10K" target="_blank">ARPO-RL-Reasoning-10K</a>
ü§ó <a href="https://huggingface.co/datasets/dongguanting/ARPO-RL-DeepSearch-1K" target="_blank">ARPO-RL-DeepSearch-1K</a>
</p>
-->



<h5 align="center"> If you like our project, please give us a star ‚≠ê on GitHub for the latest update.</h5>

## üì£ Latest News

- **[July 25, 2025]**: üî• We released all our **ARPO model checkpoints (3B~14B)** and **datasets(SFT, RL, Evaluation)**. Checkout **[ü§óARPO Collection](https://huggingface.co/collections/dongguanting/arpo-688229ff8a6143fe5b4ad8ae)** here. We will keep update it!
- **[July 25, 2025]**: üìÑ Our paper is now available on **[arXiv]()** and **[Hugging Face]()** daily paper.
- **[July 25, 2025]**: üöÄ Full codebase released. ARPO supports multi-tool agentic RL for the Qwen3 and Llama3 series models. Our team has implemented extensive parallelization and memory optimization during RL training. 


## Table of Contents


- [Overall Performance](#-overall-performance)
- [Quick Start](#-quick-start-for-training)
  - [Cold-Start SFT Stage](#-cold-start-sft-stage)
    - [Environment Setup](#1-environment-setup)
    - [Fine-Tuning Model](#2-fine-tuning-model)
  - [Self-Critic RL Stage](#-self-critic-rl-stage)
    - [Environment Setup](#1-environment-setup-1)
    - [Vanilla RL Training](#2-vanilla-rl-training)
    - [Optional: Self-Critic DPO Training](#3-self-critic-dpo-training-optional)
  - [TIR Evaluation](#-tir-evaluation)
    - [Environment Setup](#1-environment-setup-2)
    - [LLM Service Deployment](#2-llm-service-deployment)
    - [Retriever Serving Deployment](#3-retriever-serving-deployment)
    - [Inference Your Model](#4-inference-your-model)
    - [Calculate Metrics](#5-calculate-metrics)
  - [Performance of Tool-Star Models](#-performance-of-tool-star-models)
- [Citation](#-citation)




## üí° Overview

We propose **Agentic Reinforced Policy Optimization (ARPO)**, **an agentic RL algorithm tailored for training multi-turn LLM-based agent**. The core principle of ARPO is to encourage the policy model to adaptively branch sampling during high-entropy tool-call rounds, thereby efficiently aligning step-level tool-use behaviors.

<img width="1015" height="487" alt="image" src="https://github.com/user-attachments/assets/6c43dcdf-b47f-4066-a62c-4dd25b3e3356" />


- In figure (left), The initial tokens generated by the LLM after receiving **each round of tool-call feedback consistently exhibit a high entropy**. This indicates that external tool-call significantly **introduces uncertainty into the LLM‚Äôs reasoning process**.

- In the figure (right), we validate ARPO's performance **across 13 datasets**. Notably, Qwen3-14B with ARPO excelled in Pass@5, **achieving 61.2% on GAIA and 24.0% on HLE**, while requiring only about **half the tool calls** compared to GRPO during training.






## üèÉ Quick Start

Reproducing ARPO requires three steps: cold start fine-tuning, ARPO training, and evaluation. Below, we will provide a detailed explanation.

## ‚ùÑÔ∏è Cold-Start SFT Stage

### 1. Environment Setup

In this step, we will describe how to perform a cold start for the SFT stage using the LLaMA Factory repository. First, set up the environment as follows:

```bash
# Clone the ARPO repository (which includes LLaMA-Factory)
git clone https://github.com/dongguanting/ARPO
cd ARPO/LLaMA-Factory

# Create a new conda environment
conda create -n sft python=3.10
conda activate sft

# Install dependencies
pip install -r requirements.txt
```

### 2. Fine-Tuning Model


1. Download your SFT dataset from [ü§óARPO-SFT-54K](https://huggingface.co/datasets/dongguanting/ARPO-SFT-54K) and place it in `LLaMA-Factory-main/data/final_sft_edition9.json`. Define the dataset in `dataset_info.json`.

2. Complete the path information in `LLaMA-Factory/arpo_train_sft/yaml`. The file content should be as follows:

```yaml
### model
model_name_or_path: your_model_path/Qwen3-14B
trust_remote_code: true

### method
stage: sft
do_train: true
finetuning_type: full
deepspeed: ../examples/deepspeed/ds_z3_config.json  # choices: [ds_z0_config.json, ds_z2_config.json, ds_z3_config.json]

### dataset
dataset_dir: dataset_info
dataset: your_dataset
template: qwen
cutoff_len: 15000
max_samples: 1000000
overwrite_cache: true
preprocessing_num_workers: 16

### output
output_dir: checkpoints/qwen
logging_steps: 10
save_steps: 2000
plot_loss: true
overwrite_output_dir: true

### train
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
learning_rate: 7.0e-6
num_train_epochs: 3.0
lr_scheduler_type: cosine
warmup_ratio: 0.1
bf16: true
ddp_timeout: 180000000

```

After completing the information, you can fine-tune the model using the following command:

```python
bash arpo_train_sft/sft_train.sh
```

---

## üî• ARPO Stage

In this step, we will load the cold-start data for GRPO training. We reference the [ReCall](https://github.com/Agent-RL/ReCall) and [VERL](https://github.com/volcengine/verl) frameworks for RL training.


### 1. Environment Setup

 you can install our additional environment as follow: 

```bash
#create env
conda create -n arpo python==3.10
conda activate arpo

# install torch & flash-atten
pip3 install torch==2.4.0 --index-url https://download.pytorch.org/whl/cu124
pip3 install flash-attn --no-build-isolation

# install RL basic env
cd arpo
pip3 install -e .

# This is our RL env freeze file. You can install it as a supplement or use it for checking.
pip install -r requirements.txt

```


### 2. ARPO RL Training

üöÄ We will supplement ARPO training as soon as possible, Expected by July 27th !

---

## ‚úÖ ARPO Evaluation

If you have already trained a model, you can refer to the following process for TIR capability evaluation. Of course, you can also download our checkpoint from **[ü§óARPO-Huggingface-Collection](https://huggingface.co/collections/dongguanting/arpo-688229ff8a6143fe5b4ad8ae)** for directly testing.
This guide walks you through setting up two separate environments:
- One for **vLLM inference service** (`vllm_env`)
- One for **evaluation pipeline** (`evaluation`)

### 1. Setup vLLM Inference Environment

```bash
# Step into the vllm_scripts directory
cd evaluation/vllm_scripts

# Create a dedicated conda environment for vLLM
conda create -n vllm_env python=3.10
conda activate vllm_env

# Install dependencies (edit as needed)
pip install -r requirements.txt
```

Edit the following launch scripts with your own model paths and names:

In `vllm_launch_reasoning_model_cuda4-7.sh`:
```bash
MODEL_PATH="<path/to/your/reasoning_model_checkpoint>"
MODEL_NAME="your_model_name"
```

For summarization models (choose one):
```bash
MODEL_PATH="<path/to/your/summarization_model_checkpoint>"
MODEL_NAME="your_summarization_model_name"
```

Launch the vLLM services:
```bash
# Start the reasoning model
bash vllm_launch_reasoning_model_cuda4-7.sh

# Start the summarization model (choose one)
bash vllm_launch_summarize_model_cuda0-3_<your_model>.sh
```

---

### 2. Setup Evaluation Environment

```bash
# Create a separate environment for evaluation
conda create -n evaluation python=3.10
conda activate evaluation

# Install required packages
cd evaluation
pip install -r requirements.txt
```

---

### 3. Configure and Run Evaluation

Edit the `infer_local_sds.sh` script with the following values:

```bash
# Activate your Conda environment manually if 'conda' is not available in shell
source < /path/to/your/conda >/bin/activate
conda activate < your env name >

# Datasets to evaluate ‚Äî uncomment the ones you want to include:
# Options: aime24, aime25, math500, gsm8k, math, webwalker, hotpotqa, 2wiki, bamboogle, musique, hle, gaia, SimpleQA, xbench
data_names=(
    "hle"
    "gaia"
)

# Required parameters to update:
EXP_NAME="<your_exp_name>"                   # Name of this experiment run
MODEL_PATH="<your_model_path>"               # Path to the reasoning model
OUTPUT_PATH="<your_output_path>"             # Directory to save outputs
CONDA_PATH="<your_conda_path>"               # Path to your Conda installation
CONDA_ENV="<your_env_name>"                  # Name of your Conda environment
BING_API_KEY="<your_bing_search_api_key>"    # Bing Search API key
BING_ZONE="<your_bing_zone>"                 # Bing API zone
SUMM_MODEL_PATH="<your_summarization_model_path>"  # Path to summarization model checkpoints
```

Run the evaluation:
```bash
bash evaluation/infer_local_sds.sh
```

> üî∏ For Chinese datasets like `xbench`, use `infer_local_sds_cn.sh` instead.


### 4. Calculate Metrics

After generating inference results, you can use a large model like **Qwen2.5-72B-Instruct** to evaluate them with more powerful understanding capabilities.

First, use the vLLM environment to start the evaluation model:

```bash
bash evaluation/deploy_qwen2.5_72B_instruct.sh
```

In that script, make sure to update the `vllm serve` command with your own model path:

```bash
# Activate your Conda environment manually if 'conda' is not available in shell
source < /path/to/your/conda >/bin/activate
conda activate < your env name >

vllm serve <your_model_path> \
  --served-model-name Qwen2.5-72B-Instruct \
  --max-model-len 32768 \
  --tensor_parallel_size 4 \
  --gpu-memory-utilization 0.75 \
  --quantization gptq \
  --port 8001
```

Before running the evaluation script, update the following line in `evaluate_passk.sh` to specify the output directory:

```bash
OUTPUT_DIR="<your_result_directory>"
```

Then, run the evaluation script to calculate metrics:

```bash
bash evaluation/evaluate_passk.sh
```
---


## üìÑ Citation

If you find this work helpful, please cite our paper:
```bibtex

```

## ü§ù Acknowledge

This training implementation builds upon [Tool-Star](https://github.com/dongguanting/Tool-Star), [Llama Factory](https://github.com/hiyouga/LLaMA-Factory), [verl](https://github.com/volcengine/verl) and [ReCall](https://github.com/Agent-RL/ReCall). For evaluation, we rely on [WebThinker](https://github.com/RUC-NLPIR/WebThinker), [HIRA](https://github.com/RUC-NLPIR/HiRA), [WebSailor](https://github.com/Alibaba-NLP/WebAgent), [Search-o1](https://github.com/sunnynexus/Search-o1), and [FlashRAG](https://github.com/RUC-NLPIR/FlashRAG). The Python interpreter design references [ToRA](https://github.com/microsoft/ToRA) and [ToRL](https://github.com/GAIR-NLP/ToRL), while our models are trained using [Qwen2.5](https://qwenlm.github.io/blog/qwen2.5/). We express our sincere gratitude to these projects for their invaluable contributions to the open-source community. 


## üìÑ License

This project is released under the [MIT License](LICENSE).

## üìû Contact

For any questions or feedback, please reach out to us at [dongguanting@ruc.edu.cn](dongguanting@ruc.edu.cn).


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=dongguanting/ARPO&type=Date)](https://www.star-history.com/#dongguanting/ARPO&Date)
